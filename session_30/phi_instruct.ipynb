{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhairyashil/miniconda3/envs/erav2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Set device to MPS if available, otherwise CPU\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_clip():\n",
    "    # -- processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    # Load CLIP processor and model, saving locally if not already present\n",
    "    if os.path.exists(\"data/processor_clip_embeddings_vit_base_patch32.pt\"):\n",
    "        processor = CLIPProcessor.from_pretrained(\"data/processor_clip_embeddings_vit_base_patch32.pt\")\n",
    "    else:\n",
    "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        # save processor for later use\n",
    "        processor.save_pretrained(\"data/processor_clip_embeddings_vit_base_patch32.pt\")\n",
    "\n",
    "    model_clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "    model_clip = model_clip.to(device).to(torch.float32)\n",
    "    return processor, model_clip\n",
    "\n",
    "def generate_clip_embeddings(processor, model_clip, image_dir, embeddings_path):\n",
    "    # Generate CLIP embeddings for images and save them\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    clip_embeddings = []\n",
    "    image_ID = {}\n",
    "\n",
    "    for i, image_file in enumerate(image_files[:100]):  # Limit to 100 images for this example\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        image = Image.open(image_path)\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            image_features = model_clip.get_image_features(**inputs)\n",
    "        clip_embeddings.append(image_features.cpu())\n",
    "        image_ID[image_file.split(\".\")[0]] = i\n",
    "\n",
    "    torch.save(torch.cat(clip_embeddings, dim=0), embeddings_path)\n",
    "    return image_ID\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "    # Linear projection layer to map CLIP embeddings to Phi model dimensions\n",
    "    def __init__(self, clip_embedding_dim, phi_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(clip_embedding_dim, phi_hidden_dim)\n",
    "\n",
    "    def forward(self, image_embeddings):\n",
    "        return self.linear(image_embeddings)\n",
    "\n",
    "class MultimodalPhiWithAdapter(nn.Module):\n",
    "    # Multimodal model combining Phi language model with image embeddings\n",
    "    def __init__(self, language_model, projection_layer, freeze_language_model=True, freeze_projection_layer=False):\n",
    "        super().__init__()\n",
    "        self.language_model = language_model\n",
    "        self.projection_layer = projection_layer\n",
    "        \n",
    "        # Set trainable parameters based on input flags\n",
    "        self.set_trainable_params(freeze_language_model, freeze_projection_layer)\n",
    "\n",
    "    def set_trainable_params(self, freeze_language_model, freeze_projection_layer):\n",
    "        # Set which parts of the model are trainable\n",
    "        for param in self.language_model.parameters():\n",
    "            param.requires_grad = not freeze_language_model\n",
    "        for param in self.projection_layer.parameters():\n",
    "            param.requires_grad = not freeze_projection_layer\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image_embeddings, labels=None):\n",
    "        # Forward pass combining image embeddings with text input\n",
    "        batch_size = input_ids.shape[0]\n",
    "        projected_embeddings = self.projection_layer(image_embeddings)\n",
    "        \n",
    "        # Prepend projected image embeddings to the input sequence\n",
    "        input_embeds = self.language_model.get_input_embeddings()(input_ids)\n",
    "        combined_embeds = torch.cat([projected_embeddings.unsqueeze(1), input_embeds], dim=1)\n",
    "        combined_embeds = combined_embeds.to(torch.float32)\n",
    "        \n",
    "        # Adjust attention mask and labels for the added image token\n",
    "        image_attention = torch.ones((batch_size, 1), dtype=torch.long, device=device)\n",
    "        combined_attention_mask = torch.cat([image_attention, attention_mask], dim=1)\n",
    "\n",
    "        # Adjust labels\n",
    "        if labels is not None:\n",
    "            # Add a padding label for the image token\n",
    "            pad_labels = torch.full((batch_size, 1), -100, dtype=labels.dtype, device=labels.device)\n",
    "            labels = torch.cat([pad_labels, labels], dim=1)\n",
    "        \n",
    "        outputs = self.language_model(inputs_embeds=combined_embeds, attention_mask=combined_attention_mask)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Calculate loss if labels are provided\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            shift_logits = logits[:, 1:, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "    \n",
    "    # return total number of trainable parameters\n",
    "    def count_trainable_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def count_total_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "class InstructDataset(torch.utils.data.Dataset):\n",
    "    # Dataset class for instruction-following data with images\n",
    "    def __init__(self, instruct_data, clip_embeddings, tokenizer, image_ID, max_length=512):\n",
    "        self.instruct_data = instruct_data\n",
    "        self.clip_embeddings = clip_embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.image_ID = image_ID\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instruct_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Prepare a single item from the dataset\n",
    "        item = self.instruct_data[idx]\n",
    "        conversations = item['conversations']\n",
    "        \n",
    "        full_text = \"\\n\".join([f\"{conv['from']}: {conv['value']}\" for conv in conversations])\n",
    "        \n",
    "        image_id = item[\"id\"]\n",
    "        img_idx = self.image_ID.get(image_id, 0)  # Default to first embedding if not found\n",
    "        image_embedding = self.clip_embeddings[img_idx]\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            full_text, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoded.input_ids.squeeze(),\n",
    "            \"attention_mask\": encoded.attention_mask.squeeze(),\n",
    "            \"image_embeddings\": image_embedding,\n",
    "            \"labels\": encoded.input_ids.squeeze(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor, model_clip = load_clip()\n",
    "image_dir = \"data/images_train2017\"\n",
    "embeddings_path = \"data/clip_embeddings.pt\"\n",
    "image_ID = generate_clip_embeddings(processor, model_clip, image_dir, embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clip_embeddings = torch.load(embeddings_path)\n",
    "clip_embedding_dim = clip_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.04s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer loaded from local\n"
     ]
    }
   ],
   "source": [
    "# Load your Phi model\n",
    "if os.path.exists(\"local_phi2_model\"):\n",
    "    model_phi = AutoModelForCausalLM.from_pretrained(\"local_phi2_model\").to(device)\n",
    "else:\n",
    "    model_phi = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\").to(device)\n",
    "    model_phi.save_pretrained(\"local_phi2_model\")\n",
    "\n",
    "# -- tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
    "if os.path.exists(\"local_phi2_model\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"local_phi2_model\")\n",
    "    print(\"tokenizer loaded from local\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
    "    tokenizer.save_pretrained(\"local_phi2_model\")\n",
    "    print(\"tokenizer loaded from HF\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "projection_layer = ProjectionLayer(clip_embedding_dim, model_phi.config.hidden_size).to(device)\n",
    "multimodal_phi = MultimodalPhiWithAdapter(model_phi, projection_layer).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhairyashil/miniconda3/envs/erav2/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configure QLoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    #task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "multimodal_phi = get_peft_model(multimodal_phi, lora_config)\n",
    "\n",
    "# After applying LoRA, if you want to unfreeze certain parts:\n",
    "multimodal_phi.set_trainable_params(freeze_language_model=True, freeze_projection_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 2,782,307,840\n",
      "Projection layer parameters: 1,313,280\n",
      "Adapter (trainable) parameters: 1,313,280\n",
      "Percentage of trainable parameters: 0.05%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add this new code block\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "total_params = count_parameters(multimodal_phi)\n",
    "trainable_params = count_trainable_parameters(multimodal_phi)\n",
    "projection_params = count_parameters(projection_layer)\n",
    "adapter_params = trainable_params  # Since only adapter layers are trainable\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Projection layer parameters: {projection_params:,}\")\n",
    "print(f\"Adapter (trainable) parameters: {adapter_params:,}\")\n",
    "print(f\"Percentage of trainable parameters: {(adapter_params / total_params) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruct data loaded from local\n",
      "instruct data loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -- instruct_data = load_dataset(\"liuhaotian/LLaVA-Instruct-150K\", split='train')\n",
    "# Check for local copy first\n",
    "if os.path.exists(\"model_instruct150k\"):\n",
    "    instruct_data = load_dataset(\"model_instruct150k\", split='train')\n",
    "    print(\"instruct data loaded from local\")\n",
    "else:\n",
    "    print(\"loading instruct data from HF\")\n",
    "    instruct_data = load_dataset(\"liuhaotian/LLaVA-Instruct-150K\", split='train')\n",
    "    # Save the dataset locally for future use\n",
    "    instruct_data.save_to_disk(\"model_instruct150k\")\n",
    "print(\"instruct data loaded\")\n",
    "\n",
    "instruct_data = instruct_data.filter(lambda x: x['id'] in image_ID.keys())\n",
    "train_dataset = InstructDataset(instruct_data, clip_embeddings, tokenizer, image_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [13:42<00:00,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 822.0464, 'train_samples_per_second': 1.146, 'train_steps_per_second': 0.146, 'train_loss': 10.175467936197917, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=10.175467936197917, metrics={'train_runtime': 822.0464, 'train_samples_per_second': 1.146, 'train_steps_per_second': 0.146, 'total_flos': 0.0, 'train_loss': 10.175467936197917, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=False,\n",
    "    bf16=False,  # MPS doesn't support bfloat32\n",
    "    tf32=False,\n",
    "    half_precision_backend=\"auto\",\n",
    ")\n",
    "\n",
    "multimodal_phi = multimodal_phi.to(torch.float32)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=multimodal_phi.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only the LoRA state dict\n",
    "multimodal_phi.save_pretrained(\"fine_tuned_phi_lora\", state_dict=multimodal_phi.state_dict())\n",
    "# save projection layer\n",
    "torch.save(projection_layer.state_dict(), \"projection_layer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()\n",
    "    \n",
    "# Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\")\n",
    "\n",
    "# Load the LoRA configuration\n",
    "peft_config = PeftConfig.from_pretrained(\"fine_tuned_phi_lora\")\n",
    "\n",
    "projection_layer = ProjectionLayer(clip_embedding_dim, model_phi.config.hidden_size)\n",
    "projection_layer.load_state_dict(torch.load(\"projection_layer.pt\"))\n",
    "\n",
    "# Load the fine-tuned model\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model, \"fine_tuned_phi_lora\")\n",
    "\n",
    "# use base, projection layer, and fine tuned model to make whole model\n",
    "whole_model = MultimodalPhiWithAdapter(base_model, projection_layer)\n",
    "whole_model.load_state_dict(torch.load(\"fine_tuned_phi_lora.pt\"))\n",
    "# use whole model to generate text\n",
    "inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").to(device)\n",
    "whole_model.generate(**inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erav2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
